{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "import scipy\n",
    "import re\n",
    "\n",
    "class ResponseAI(object):\n",
    "\n",
    "    def __init__(self):\n",
    "        self.model = self.loadGloveModel(\"./data/glove.42B.300d.txt\")\n",
    "        self.freq_dict = self.build_frequency_dictionary()\n",
    "        self.faq_vectors = [] #This is a list of SENTENCE VECTORS\n",
    "        self.DICT_CONSTANT = len(self.freq_dict)\n",
    "        self.yes_answers = self.clean_sentence([\"got it\", \"awesome\", \"cool\", \"amazing\", \"nice\", \"yes\", \"go\", \"go on\", \"continue\", \"got it\", \"sure\", \"move\", \"move on\", \"ready\", \"onward\", \"done\", \"got it\", \"next\", \"sounds good\", \"understood\", \"good\"])\n",
    "        self.no_answers = self.clean_sentence([\"huh\", \"why\", \"what\", \"when\", \"where\", \"wait\", \"confused\", \"question\", \"show\", \"possible\", \"sorry\", \"stop\", \"no\"])\n",
    "        self.yes_vectors = np.squeeze(np.array([self.get_sentencevec(ans) for ans in self.yes_answers]))\n",
    "        self.no_vectors = np.squeeze(np.array([self.get_sentencevec(ans) for ans in self.no_answers]))\n",
    "        self.set_base_faq_sentencevecs()\n",
    "\n",
    "    # solve glove model loading time\n",
    "    def loadGloveModel(self, gloveFile):\n",
    "        f = open(gloveFile,'r')\n",
    "        model = {}\n",
    "        count = 0\n",
    "        for line in f:\n",
    "            count += 1\n",
    "            splitLine = line.split()\n",
    "            word = splitLine[0]\n",
    "            embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "            model[word] = embedding\n",
    "        return model\n",
    "\n",
    "    def clean_sentence(self, docs):\n",
    "        stoplist = set('for a of the and to in'.split())\n",
    "        text = [[word for word in document.lower().split() if word not in stoplist]\n",
    "            for document in docs]\n",
    "        return text\n",
    "\n",
    "    def build_frequency_dictionary(self):\n",
    "        word_frequency_data = [line.rstrip('\\n') for line in open('./data/20k.txt')]\n",
    "        word_frequency_data = {k: v for v, k in enumerate(word_frequency_data)}\n",
    "        return word_frequency_data\n",
    "\n",
    "    def set_base_faq_sentencevecs(self):\n",
    "        lines = [line.rstrip('\\n') for line in open('./data/faq.txt')]\n",
    "        for line in lines:\n",
    "            line = self.clean_line(line)\n",
    "            self.faq_vectors.append(self.get_sentencevec(line))\n",
    "\n",
    "    def clean_line(self, line_to_clean):\n",
    "        common_words = [line.rstrip('\\n').lower() for line in open('./data/common-50.txt')]\n",
    "        line_to_clean = re.sub('[^a-zA-Z ]', '', line_to_clean)\n",
    "        line_to_clean = [word for word in line_to_clean.lower().split()]\n",
    "        cleaned_line = []\n",
    "        for word in line_to_clean:\n",
    "            if word not in common_words:\n",
    "                cleaned_line.append(word)\n",
    "        return cleaned_line\n",
    "\n",
    "    def get_weight(self, word):\n",
    "        try:\n",
    "            return self.DICT_CONSTANT - self.freq_dict[word] #The idea is to reverse the indexing so less common words = more important.\n",
    "        except:\n",
    "            return self.DICT_CONSTANT\n",
    "\n",
    "    def get_sentencevec(self, line):\n",
    "        #Sentence vector is sum of (wordvec for each word)*(1/1+f), where f is frequency.\n",
    "        sentence_vec = []\n",
    "        for word in line:\n",
    "            multiplier = (float(1e7)/float(1+self.get_weight(word)))\n",
    "            wordvec = np.array([self.word_to_vec(word)])\n",
    "            weighted_wordvec = multiplier*wordvec\n",
    "            sentence_vec.append(weighted_wordvec)\n",
    "        sentence_vec = np.sum(sentence_vec, axis=0)\n",
    "        sentence_vec /= float(len(line))\n",
    "        return sentence_vec\n",
    "\n",
    "    def word_to_vec(self, word):\n",
    "        wordvec_list = []\n",
    "        # word = filter(str.isalpha, word)\n",
    "        try:\n",
    "            wordvec = self.model[word] # e.g. wordvec[\"emily\"] = [3,5,524,234]\n",
    "            wordvec_list.append(wordvec)\n",
    "        except:\n",
    "            pass\n",
    "        if not wordvec_list:\n",
    "            return np.zeros(300)\n",
    "        data = np.array(wordvec_list)\n",
    "        vec = data.mean(axis=0)\n",
    "        return vec\n",
    "\n",
    "    def find_shortest(self, question_sentencevec):\n",
    "        min_values = []\n",
    "        for i in range(len(self.faq_vectors)):\n",
    "            min_values.append(scipy.spatial.distance.euclidean(question_sentencevec, self.faq_vectors[i]))\n",
    "        return np.argmin(min_values), np.min(min_values)\n",
    "\n",
    "    # # message is an array\n",
    "    # def sentence2vec(self, message):\n",
    "    #     wordvec_list = []\n",
    "    #     for word in message:\n",
    "    #         # word = filter(str.isalpha, word)\n",
    "    #         try:\n",
    "    #             wordvec = self.model[word] # e.g. wordvec[\"emily\"] = [3,5,524,234]\n",
    "    #             wordvec_list.append(wordvec)\n",
    "    #         except:\n",
    "    #             pass\n",
    "    #     if not wordvec_list:\n",
    "    #         return np.zeros(300)\n",
    "    #\n",
    "    #     data = np.array(wordvec_list)\n",
    "    #     vec = data.mean(axis=0)\n",
    "    #     return vec\n",
    "\n",
    "    def decide_question(self, question):\n",
    "        question = self.clean_line(question)\n",
    "        question_sentencevec = self.get_sentencevec(question)\n",
    "        result, confident = self.find_shortest(question_sentencevec)\n",
    "        return result, confident\n",
    "\n",
    "    def intentClassifier(self, message):\n",
    "        print(message)\n",
    "        message_vec = self.get_sentencevec(message.split(' '))\n",
    "\n",
    "        # no_dots = self.no_vectors.dot(message_vec)\n",
    "        # max_no = no_dots.max()\n",
    "        no_scores = np.matmul(message_vec, self.no_vectors.T)\n",
    "        max_no = np.max(no_scores)\n",
    "\n",
    "        # yes_dots = self.yes_vectors.dot(message_vec)\n",
    "        # max_yes = yes_dots.max()\n",
    "        yes_scores = np.matmul(message_vec, self.yes_vectors.T)\n",
    "        max_yes = np.max(yes_scores)\n",
    "\n",
    "        print(max_yes > max_no)\n",
    "        return max_yes > max_no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "responseAI = ResponseAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Move on\n",
      "True\n",
      "Got it\n",
      "True\n",
      "Yeah, I'm good with that one\n",
      "True\n",
      "I'm ready to move on\n",
      "True\n",
      "Sounds good!\n",
      "False\n",
      "Ya man. sounds good\n",
      "False\n",
      "I had no difficulty understanding that\n",
      "False\n",
      "i got it, bro\n",
      "False\n",
      "alright\n",
      "False\n",
      "not sure about that\n",
      "False\n",
      "i don't feel perfectly confident\n",
      "False\n",
      "there needs to be more explanation\n",
      "False\n",
      "what is that Ibiza thing about?\n",
      "False\n",
      "i need some help\n",
      "False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "responseAI.intentClassifier(\"Move on\")\n",
    "responseAI.intentClassifier(\"Got it\")\n",
    "responseAI.intentClassifier(\"Yeah, I'm good with that one\")\n",
    "responseAI.intentClassifier(\"I'm ready to move on\")\n",
    "responseAI.intentClassifier(\"Sounds good!\")\n",
    "responseAI.intentClassifier(\"Ya man. sounds good\")\n",
    "responseAI.intentClassifier(\"I had no difficulty understanding that\")\n",
    "responseAI.intentClassifier(\"i got it, bro\")\n",
    "responseAI.intentClassifier(\"alright\")\n",
    "responseAI.intentClassifier(\"not sure about that\")\n",
    "responseAI.intentClassifier(\"i don't feel perfectly confident\")\n",
    "responseAI.intentClassifier(\"there needs to be more explanation\")\n",
    "responseAI.intentClassifier(\"what is that Ibiza thing about?\")\n",
    "responseAI.intentClassifier(\"i need some help\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
